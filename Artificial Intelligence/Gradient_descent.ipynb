{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xAlF1UKw9cXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# The given function\n",
        "def f(x, y):\n",
        "  return 5*x**2 + 40*x + y**2 - 12*y + 127\n",
        "\n",
        "    # The gradient of the function\n",
        "def gradient(x, y):\n",
        "  dfdx = 10*x + 40\n",
        "  dfdy = 2*y -12\n",
        "  return np.array([dfdx, dfdy])\n",
        "\n",
        "# Gradient Descent algorithm\n",
        "def gradient_descent(lr, steps):\n",
        "  x0 = np.random.uniform(-10,10,2)\n",
        "  for _ in range(steps):\n",
        "    grad = gradient(x0[0], x0[1])\n",
        "    x0 -= lr * grad\n",
        "    return f(x0[0], x0[1]), x0\n",
        "\n",
        "learning_rates = [0.1, 0.01, 0.001]\n",
        "steps =500\n",
        "trials=10\n",
        "\n",
        "\n",
        "for lr in learning_rates:\n",
        "  best_performance = float('inf')\n",
        "\n",
        "  for _ in range(trials):\n",
        "    performance,x_min=gradient_descent(lr , steps)\n",
        "\n",
        "    if performance < best_performance:\n",
        "      best_performance=performance\n",
        "\n",
        "  print(f\"Learning Rate: {lr}, Minimum Value: {best_performance}, at point {x_min}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYPXm7je9c9R",
        "outputId": "43af95f8-9c4f-4d36-9f12-b92a775c89f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning Rate: 0.1, Minimum Value: 11.263239464005693, at point [-4.          0.32255877]\n",
            "Learning Rate: 0.01, Minimum Value: 53.13810805150263, at point [6.19504126 8.19152701]\n",
            "Learning Rate: 0.001, Minimum Value: 25.17037109296345, at point [6.78958835 0.66586585]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ySFHzSgc9dAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# The given function\n",
        "def f(x, y):\n",
        "  return 5*x**2 + 40*x + y**2 - 12*y + 127\n",
        "\n",
        "    # The gradient of the function\n",
        "def gradient(x, y):\n",
        "  dfdx = 10*x + 40\n",
        "  dfdy = 2*y -12\n",
        "  return np.array([dfdx, dfdy])\n",
        "\n",
        "\n",
        "learningrate =[0.1, 0.01, 0.001]\n",
        "steps = 500\n",
        "trials = 10\n",
        "\n",
        "def gradientdescent(lr,steps ):\n",
        "  #randomize xo\n",
        "  x0 = np.random.uniform(-10,10,2)\n",
        "  for _ in range(steps):\n",
        "    partgrad = lr * gradient(x0[0],x0[1])\n",
        "    x0 -= partgrad\n",
        "    return f(x0[0], x0[1]), x0\n",
        "\n",
        "for lr in learningrate:\n",
        "  best_performance = float('inf')\n",
        "  for i in range(trials):\n",
        "    performance,x_min=gradientdescent(lr, steps)\n",
        "    if performance < best_performance:\n",
        "      best_performance = performance\n",
        "      best_min = x_min\n",
        "\n",
        "# Print the best performance and the best x_min\n",
        "\n",
        "  print(f\"Learning Rate: {lr}, Minimum Value: {best_performance}, at point {best_min}\")"
      ],
      "metadata": {
        "id": "OmHDNEsx7Ewk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34aa7a35-161e-4ac1-da6f-dfd5bd647eae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning Rate: 0.1, Minimum Value: 12.755033308873209, at point [-4.         4.6752233]\n",
            "Learning Rate: 0.01, Minimum Value: 17.947835787965914, at point [-4.92179716  7.64295032]\n",
            "Learning Rate: 0.001, Minimum Value: 12.025771950056424, at point [-3.67570241  6.70705539]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-Foszoek6e69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L2da1JMI6e-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NUU-j5346fJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# The given function\n",
        "def f(x, y):\n",
        "  return 5*x**2 + 40*x + y**2 - 12*y + 127\n",
        "\n",
        "    # The gradient of the function\n",
        "def gradient(x, y):\n",
        "  dfdx = 10*x + 40\n",
        "  dfdy = 2*y -12\n",
        "  return np.array([dfdx, dfdy])\n",
        "\n",
        "# Gradient Descent algorithm\n",
        "def gradient_descent(lr, steps):\n",
        "  x0 = np.random.uniform(-10,10,2)\n",
        "  for _ in range(steps):\n",
        "    grad = -1 * gradient(x0[0], x0[1])\n",
        "    x0 -= lr * grad\n",
        "    return f(x0[0], x0[1]), x0\n",
        "\n",
        "learning_rates = [0.1, 0.01, 0.001]\n",
        "steps =500\n",
        "trials=10\n",
        "\n",
        "\n",
        "for lr in learning_rates:\n",
        "  best_performance = float('inf')\n",
        "\n",
        "  for _ in range(trials):\n",
        "    performance,x_min=gradient_descent(lr , steps)\n",
        "\n",
        "    if performance < best_performance:\n",
        "      best_performance=performance\n",
        "\n",
        "  print(f\"Learning Rate: {lr}, Minimum Value: {best_performance}, at point {x_min}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ef00252-b80a-499c-eecb-a39faad61a6d",
        "id": "ocUJuqKs6kjD"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning Rate: 0.1, Minimum Value: 47.8492192546928, at point [-1.94713947  9.97215778]\n",
            "Learning Rate: 0.01, Minimum Value: 47.96391199176085, at point [-6.71885957  5.94591458]\n",
            "Learning Rate: 0.001, Minimum Value: 36.97841003073813, at point [4.8538812  1.12325167]\n"
          ]
        }
      ]
    }
  ]
}